# Importing necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

import sklearn
import sklearn.model_selection
import sklearn.ensemble

# Installing Beautiful Soup package
!pip install transformers
from transformers import pipeline
from bs4 import BeautifulSoup
import requests

# Downloading data
df_type = pd.read_csv("MBTI_type.csv")
df_posts = pd.read_csv("MBTI_posts.csv")

# Cleaning the data
# Separating the cell that contains 50 posts each into 50 columns for eash post
sep_posts = df_posts['posts'].str.split("|\|\|", expand = True) 
sample_posts = pd.DataFrame(sep_posts[0].str[1:])                       
df = pd.concat([df_type, sample_posts], axis=1)

# Checking if the data is imbalanced
df_dist = df["type"].value_counts()
df_dist.plot(kind="bar")
df_dist.set_xlabel("Personality Types")
df_dist.set_ylabel("Number of Samples")
plt.show()
plt.tight_layout()

# Creating a dataframe for each personality type
by_type = df.groupby("type")
INTJ = by_type.get_group("INTJ")
INTP = by_type.get_group("INTP")
ESTJ = by_type.get_group("ESTJ")

# Most frequently used 10 words for each personality type
from collections import counter
types = [ESTJ, ENTJ, ESFJ, ENFJ, ISTJ, ISFJ, INTJ, INFJ, ESTP, ESFP, ENTP, ENFP, ISTP, ISFP, INTP, INFP]

for type in types:
  counter = counter(type)  
  return counter.most_common(10)

# Length of each post
df_analysis = df.copy()
df_analysis['Length of post'] = df['posts'].apply(lambda x: len(x))

# Frequency of links used - counting the number of http
df_analysis['Number of links'] = df['posts'].apply(lambda x: x.count('http'))

# Frequency of punctuations used
df_analysis['Number of question marks']=df['posts'].apply(lambda x: x.count('?'))
df_analysis['Number of periods']=df['posts'].apply(lambda x: x.count('.'))
df_analysis['Number of exclamation marks']=df['posts'].apply(lambda x: x.count('!'))
df_analysis['Number of ellipsis']=df['posts'].apply(lambda x: x.count('...'))

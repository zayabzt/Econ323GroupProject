# Importing necessary libraries
import pandas as pd
import numpy as np
import sklearn
import sklearn.model_selection
import sklearn.ensemble

# Installing visualization tools
import matplotlib.pyplot as plt
%matplotlib inline
from wordcloud import WordCloud, STOPWORDS


# Installing Beautiful Soup package
!pip install transformers
from transformers import pipeline
from bs4 import BeautifulSoup
import requests


# Downloading data
df_type = pd.read_csv("MBTI_type.csv")
df_posts = pd.read_csv("MBTI_posts.csv")

# Cleaning the data
df_posts.isna().sum()
df_posts.dtypes
df_type.isna().sum()
df_type.dtypes

# Separating the cell that contains 50 posts each into 50 columns for eash post
sep_posts = df_posts['posts'].str.split("|\|\|", expand = True) 
sample_posts = pd.DataFrame(sep_posts[0].str[1:])                       
df = pd.concat([df_type, sample_posts], axis=1)

# Checking if the data is imbalanced
df_dist = df["type"].value_counts()
df_dist.plot(kind="bar")
df_dist.set_xlabel("Personality Types")
df_dist.set_ylabel("Number of Samples")
plt.show()
plt.tight_layout()

# Creating a dataframe for each personality type
by_type = df.groupby("type")
INTJ = by_type.get_group("INTJ")
INTP = by_type.get_group("INTP")
ESTJ = by_type.get_group("ESTJ")

# Most frequently used 10 words for each personality type
from collections import counter
types = [ESTJ, ENTJ, ESFJ, ENFJ, ISTJ, ISFJ, INTJ, INFJ, ESTP, ESFP, ENTP, ENFP, ISTP, ISFP, INTP, INFP]

for type in types:
  counter = counter(type)  
  return counter.most_common(10)

# Creating a wordcloud of frequenty used word for each personality type
word_cloud = WordCloud(collocations = False, background_color = 'white').generate(posts)
fig, ax = plt.subplots(16, figsize=(14, 16))
x = 0
for i in df['type'].unique():
    df_each = df[df['type'] == i]
    wordcloud = WordCloud(max_words=1000, collocations = False, background_color = 'white').generate(df_each['posts'].to_string())
    plt.subplot(3, 5, x+1)
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.title(i)
    ax[k].axis("off")
    x+=1

# Length of each post
df_analysis = df.copy()
df_analysis['Length of post'] = df['posts'].apply(lambda x: len(x))

# Frequency of links used - counting the number of http
df_analysis['Number of links'] = df['posts'].apply(lambda x: x.count('http'))

# Frequency of punctuations used
df_analysis['Number of question marks']=df['posts'].apply(lambda x: x.count('?'))
df_analysis['Number of periods']=df['posts'].apply(lambda x: x.count('.'))
df_analysis['Number of exclamation marks']=df['posts'].apply(lambda x: x.count('!'))
df_analysis['Number of ellipsis']=df['posts'].apply(lambda x: x.count('...'))
